%% LyX 2.1.0svn created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
%\usepackage{breakurl}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\makeatother

\begin{document}

% \SweaveOpts{fig.path=fig/hw1,fig.align=center,fig.show=hold,dev=png,width=4,height=3}


<<setup,echo=FALSE,results=hide,message=FALSE>>=
options(replace.assign=TRUE,width=90)
knit_hooks$set(par=function(before, options, envir){if (before) par(mar=c(4,4,.1,.1),cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3,las=1)})
require(bbmle)
require(rethinking)
require(ggplot2)
@


\begin{center}
  {\bf \Large Homework 1, Statistical Rethinking}\\
\vspace{12pt}
   {\large Jaime Ashander}
\end{center}


\subsection*{Problem 1}
\label{problem1}

First load the data, \texttt{birth1} and \texttt{birth2}, which indicate the gender (male=1, female=0) of reported first (second) born children in 100 families.
All families in this country have a maximum of two children.

Our hypothesis is that births are determined by a random process, with some probability of obtaining a boy \texttt{p.b}.
Assuming births are independent samples of this process, we model the data as random binomial trials.

\subsubsection*{1 a}
\label{a}

<<prob1a,message=FALSE>>=
data(homeworkch2) #
p.b <- seq(from=0, to=1, length.out=1000)
 prior <- rep(1/1000, 1000)
 likelihood <- dbinom(sum(birth1)+sum(birth2), size=length(c(birth1,birth2)), prob=p.b) # likelihood of data given 1000 models (binomial success parameter)
 posterior <- prior * likelihood
 posterior <- posterior/sum(posterior)
Max.post <- function(parameter, posterior){
	estimate=parameter[-log(posterior) == min(-log(posterior))]
	estimate
}
 pb.max <- Max.post(p.b, posterior) 
@


The maximimum posterior probability occurs with model: \texttt{p.b =}

<<prob1aans, message=FALSE,echo=FALSE>>=
print(pb.max)
@ 

\subsubsection*{1 b}
\label{b}

To construct highest posterior density intervals, we use the \texttt{HPDI}.
Choice of appropriate sample and formatting of output is encapsulated in function \texttt{Precis.bayes} (a modified version of \texttt{rethinking::precis}). 

<<prob1b,message=FALSE>>=
# construct table like precis
Precis.bayes<-function (parameter, posterior, param.samples=NA, level=0.95)
{
  # param.samples = an set of samples TODO: change to do resampling ?
  result <- data.frame(est=Max.post(parameter, posterior))
  colnames(result) <- c("Estimate")
  if (length(param.samples)>10){
        ci <- HPDI(param.samples, level)
        result <- cbind(result, as.data.frame(t(ci)))
    }
   result
}
p.b.sample <- sample(p.b, size=1e4, replace=TRUE, prob=posterior)
CI.types <- c(0.50, 0.90, 0.95)
CI.data <- t(sapply(CI.types,
                  function(x){unlist(Precis.bayes(p.b, posterior=posterior, param.samples=p.b.sample, level=x))}
         ))
fit.bayes <- as.data.frame(CI.data)
fit.bayes$Interval.width <- as.character(CI.types)
@ 

Credible intervals, based on sampling the posterior density, for probabilty of drawing a boy:

<<prob1btab, message=FALSE,echo=FALSE>>=
fit.bayes
@ 

Maximum posterior probability estimate of \texttt{p.b} is also reported.

\subsubsection*{1c}
\label{c}

For maximum likelihood estimation we use \texttt{mle2}, and \texttt{rethinking::precis} to construct confidence intervals. 
<<prob1c,message=FALSE>>=
births <- c(birth1, birth2)
pb.ml <-  mle2(y ~ dbinom(size=length(births), prob=pb), data=list(y=sum(births)), start=list(pb=0.5))
CI.dat.bayes <-  t(sapply(CI.types,
                        function(x){unlist(precis(pb.ml, level=x))}
           ))
fit.ml <- as.data.frame(CI.dat.bayes)
names(fit.ml)[3:4] <- c("lower", "upper")
fit.ml$Interval.width <- as.character(CI.types)
fit.ml$`Std. Error` <- NULL
@ 

Confidence intervals, based on quadratic approximation of the likelihood surface, and maximum likelihood estimate of \texttt{p.b}, probabilty of drawing a boy:
<<prob1ctab,message=FALSE,echo=FALSE>>=
fit.ml
@ 

As can be seen from the figure below, the quadratic approximation of the density agrees with the Bayesian posterior in the center of the distribution, but deviates wildly in the tails.
Additionally, the ML posterior is symmetric.

<<prob1_fig1, message=FALSE, fig=TRUE>>=
dens.compare <- data.frame(probability=p.b, Bayes=-log(posterior), ML = -log(dnorm(p.b, mean=fit.ml$Estimate[1], sd=sqrt(vcov(pb.ml)))/sum(dnorm(p.b, mean=pb.max, sd=sqrt(vcov(pb.ml))))))
d.c = melt(dens.compare, id.vars='probability')

g <- ggplot(d.c, aes(probability, value, color=variable))
g+geom_line()+ylab("- log density")
@ 


The ML confidence intervals and Bayesian HPDIs are shown below recentered by the ML estimate.
The ML intervals are symmetric, while the Bayesian intervals are slightly asymmetric.
Bayesian estimates are slightly below ML estimates, but the intervals tend to agree approximately in width.
Overall, Bayesian and ML estimate perform similarly for this data set. 

<<prob1_fig2, message=FALSE, fig=TRUE>>=
fit.bayes$type <- "Bayes (HDPI)"
fit.ml$type <- "Max. Lik. (CI)"
estimate.ml <- fit.ml[,1] # recenter by the ML estimates
fit.ml[,1:3] <- fit.ml[,1:3] - estimate.ml
fit.bayes[,1:3] <- fit.bayes[,1:3] - estimate.ml
fit.both <- rbind(fit.bayes, fit.ml)
fit.both$Interval.width <- as.factor(fit.both$Interval.width)

g <- ggplot(fit.both)
g + geom_pointrange(aes(Interval.width, Estimate, ymin=lower, ymax=upper, color=type), position=position_dodge(width=0.1))
@ 



\subsection*{Problem 2}
\label{problem2}

\subsubsection*{2a}
\label{a}

Now, we simulate the number of boys born in 200 births using \texttt{rbinom}, 10000 replicates:

<<prob2a, message=FALSE,fig=TRUE>>=
# define a function to use in simulations 
Num.boys <- function(p.b.this, n.total){
  births.this <- rbinom(n=n.total, size=1, prob=p.b.this)
  return(sum(births.this))
}

p.b.sims  <- sample(p.b, size=1e4, replace=TRUE, prob=posterior)
boys.sims <- sapply(p.b.sims, function(x){ Num.boys(x, 200)})
dens(boys.sims, adj=1)
abline(v=sum(c(birth1,birth2)),lwd=3, col='red')
@ 


Above is the density of the predicted number of boys, with the actual births (red line) right in the center of the distribution of predicted number of boys.
From this use of posterior predictive simulation, the model seems to fit the data well.

\subsubsection*{2b}
\label{b}

We now simulate the number of first born boys in 100 births using \texttt{rbinom}, 10000 replicates:

<<prob2b, message=FALSE,fig=TRUE>>=
p.b.sims  <- sample(p.b, size=1e4, replace=TRUE, prob=posterior)
fb.boys.sims <- sapply(p.b.sims, function(x){ Num.boys(x, 100)})
dens(fb.boys.sims, adj=1)
abline(v=sum(c(birth1)),lwd=3, col='red')

@ 

As above, this plot shows the density of predicted number of boys, but this time for first born boys. The actual count of first born boys (red line) is less than the center of the predicted distribution.
Here, it looks like the model overpredicts the number of first-born boys.

\subsubsection*{2c}
\label{c}

To simulate the number of second born boys with sisters as first-born, we must ask:
How many first-born girls were there, and how were the siblings born after girls distributed?
Note that the data are ordered by family.

<<prob2c,message=FALSE,echo=FALSE>>=
birth.pg <- birth2[birth1==0]
cat("first born girls: ", length(birth.pg), ", boys born after girls: ", sum(birth.pg), "\n")
@ 

So, 49 girls were born first, and following those births, 39 boys and 10 girls were born.
Now simulate 49 births and compare to acutal data (focusing as before on boys, using \texttt{rbinom}, 10000 replicates):


<<prob2c, message=FALSE,fig=TRUE>>=
p.b.sims  <- sample(p.b, size=1e4, replace=TRUE, prob=posterior)
pg.sims <- sapply(p.b.sims, function(x){ Num.boys(x, 49)})
dens(pg.sims, adj=1)
abline(v=sum(c(birth.pg)),lwd=3, col='red')
@ 


The actual number of boys born following girls (red line, 39 boys following 49 girls) exceeds by \emph{far} the center of the predicted distribution shown in the density plot above.
This could be explained if a boy is more likely to be born following a girl (non-independence).

Yet, the model performs well on births in aggregate (as shown in 2a).
This could be explained if the assumption of independence between sex of first and second born is invalid in general.
Then, if the increased likelihood of bearing girls following boys cancels out the above-demonstrated underprediction of second born boys following girls, the aggregate data could still fit our model, which assumed independence.

\subsection*{Colophon}

<<runit,eval=FALSE>>=
require(knitr) ### the package
opts_knit$set(out.format='latex')
knit('/Users/jaime/PHD/stat-rethink/hw1ashander.Rnw') ## to run
@ 


\end{document}
