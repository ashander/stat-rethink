%% LyX 2.1.0svn created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
%\usepackage{breakurl}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\makeatother

\begin{document}

% \SweaveOpts{fig.path=fig/hw2-,fig.align=center,fig.show=hold,dev=png,width=6,height=4}

<<setup,echo=FALSE,results=hide,message=FALSE>>=
options(replace.assign=TRUE,width=90)
knit_hooks$set(par=function(before, options, envir){if (before) par(mar=c(4,4,.1,.1),cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3,las=1)})
require(bbmle)
require(rethinking)
require(ggplot2)
@




\begin{center}
  {\bf \Large Homework 2, Statistical Rethinking}\\
\vspace{12pt}
   {\large Jaime Ashander}
\end{center}


\subsection*{Problem 1}
\label{problem1}

First load the data from last week, \texttt{birth1} and \texttt{birth2}, which indicate the gender (male=1, female=0) of reported first (second) born children in 100 families.
All families in this country have a maximum of two children.

Our hypothesis is that births are determined by a random process, with some probability of obtaining a boy \texttt{p.b}.
Assuming births are independent samples of this process, we model the data as random binomial trials.

\subsubsection*{1 a}
\label{1a}

Now we will compute a $p$ value for $H_0$:  {\tt p.b = 0.5}.
That is, we compute the probability we would see a result as deviant (or more deviant) assuming the null is true.

<<nhst-p1,message=FALSE>>=
data(homeworkch2) #
boys.born = sum(c(birth1,birth2))
total.births = length(c(birth1,birth2))
pb.h0 <- 0.5 #0th step: specify the null
pb.mle <- boys.born/total.births# first step: compute the observed MLE of p.b which in this case is just the proportion of boy births (more generally we could compute P(D | M) assuming we could writedown the likelihood 
# second step: compute the tail probabilities of observing a range of possible boys born from those observed to totalbirths (roughly implying pb.mle  _if the null were true_
p.b.observing <- dbinom(boys.born:total.births, size=total.births, prob = pb.h0)
# third step: compute the p value by "summing over the tail" of possible boys as great or greater (111 to 200) 
pb.pval <- sum(p.b.observing)
pb.pval
@

With $p > 0.05$, NHST suggests that we should fail to reject the null, in effect concluding that {\tt p.b = 0.5}.

\subsubsection*{1 b}
\label{1b}

The figure below reproduces the posterior from last week. The red line is {\tt p.b = 0.5}. 
In the context of our estimated posterior, the null is not a great point estimate to choose.
Although it does fall within the 90\% HPDI  (not shown) it falls outside the 50\% HPDI.

<<null-v-prior, message=FALSE,echo=FALSE,fig=TRUE>>=
p.b <- seq(from = 0, to = 1, length.out = 1000) 
prior <- rep(1/1000, 1000) 
likelihood <- dbinom(sum(birth1) + sum(birth2), size = length(c(birth1,
birth2)), prob = p.b) # likelihood of data given 1000 models (binomial success parameter)
posterior <- prior * likelihood 
posterior <- posterior/sum(posterior)
plot(posterior~p.b, type='l')
abline(v=pb.h0, col='red', lwd=2)

@ 


\subsection*{Problem 2}
\label{problem2}


The question: is a die loaded such that $Pr(die =1) = 1/2$ instead of typical $Pr(die=1) = 1/6$?

The data: 65 tosses, with 18 ``1''s. 

We model the data as random binomial trials, with the rolls assumed independent. 
Define a roll of 1 as a success, and let the probabilty of success {\tt pd}. 

<<die-data,message=FALSE>>=
die.succ <- 18
die.trials <- 65
@ 


\subsubsection*{2a}
\label{2a}

Using null hypotehsis signficance test, we compare the observed data against a null hypothesis: 

$H_0: p.d = 1/6$

As in Problem 1, we compute the $p$ value by assuming the null: 

<<die-pval,message=FALSE>>=
pd.h0 <- 1/6
pd.mle <- die.succ/die.trials
probability.under.null <- dbinom(die.succ:die.trials, size=die.trials, prob=pd.h0)
pd.pval <- sum(probability.under.null)
pd.pval
@ 

With $p = 0.017< 0.05$, NHST suggests we should reject the null in favor of the alternative hypothesis: {\tt p.d > 1/6}. 

So the die is not fair!

\subsubsection*{2b}

To use the Bayesian suggestion, we apply Bayes' theorem.

\[
Pr( fair | data)  = \frac{Pr(data | fair)Pr(fair)}{Pr(data|fair)Pr(fair)+ Pr(data|loaded)Pr(loaded)}
\]

Here, as before, the data are 18 rolls of ``1'' in 65 tries. 
The main difference is now we are assuming two models, ``fair'' and ``loaded''

<<bayes-die, message=FALSE>>=
models <- c(fair = 1/6, loaded = 0.5)
prior <- c(fair = 0.5, loaded = 0.5)
pr.data_models <- dbinom(die.succ, size=die.trials, prob = models)
pr.data <- sum(pr.data_models* prior)
pr.models_data <- pr.data_models* prior/pr.data
pr.models_data

@ 


By this analysis, there is $\approx 0.98$ probability that the die is fair. 
Quite a different result than NHST. 
If we're assuming only these two models, we should conclude the die is fair.
(So long as we're willing to risk a 2\% chance of being wrong.)

But  we still have reason to be nervous. 
Our choice to use only two models may have influenced the one we choose in the Bayesian procedure. 

One way to check is to assess how our observation compares to expected variabilty under our inference about the die,  we can use posterior predictive simulation.

<<flat-pps, message=FALSE,fig=TRUE>>=
pr.data_fair <- pr.data_models[1]
pr.data_loaded <- pr.data_models[2]
prob.samples = sample(models, size=1e6, replace=TRUE, prob=pr.models_data)
ones.sims = rbinom(1e6, size=die.trials, prob=prob.samples)
dens(ones.sims,adj=5, xlab='ones rolled')
abline(v=18)
abline(v=HPDI(ones.sims, 0.95), lty=2)
abline(v=HPDI(ones.sims, 0.97), lty=3)
@ 
The red line is our observation, which falls outside the 0.95 (dashed) density interval and right on the edge of the 0.97 (dotted) interval. 

Assuming the posterior probability given to fair in the ``two model'' Bayesian test, the observation is an extereme event. 
In some sense, our high posterior owes to the fact that our observation would be {\em much more unlikely} if the die were loaded to even odds of rolling a one.

To construct a more conservative Bayesian test, we could proceed by calculating the posterior assuming a flat prior over all possible models. 
This distribution should have a peak at $\tfrac{18}{65}$, but it's width could  help inform our final decision.

<<flat-check, message=FALSE, fig=TRUE>>=
pri = rep(1/1000, 1000)
mdls = seq(0, 1, length.out=1000)
lik = dbinom(die.succ, die.trials, prob=mdls)
post = pri*lik
post = post/sum(post)
plot(post~mdls, type='l', xlab="models", ylab="posterior")
abline(v=1/6, col='red')
hella.samples = sample(mdls, size=1e6, replace=TRUE, prob=post)
abline(v = HPDI(hella.samples, 0.95), lty=2)
abline(v = HPDI(hella.samples, 0.99), lty=3)
@ 
The red is a fair die, which falls between the 0.95 (dashed) and 0.99 (dotted) HPDI intervals. 
This procedure suggests that the posterior probability of fairness is rather low. 

Overall, the above results show that our assumptions about the prior and number of models are influencing the results of our tests.
This suggest we have insufficient data to conclude one way or another on the fairness of the die. 
If possible, we should attempt to collect more data. 

Of course, if we have very strong reasons to assume only two models, then we should still conclude the die is fair. 
For example, if we know that the only type of loaded die widely manufactured has even odds of rolling a one \dots 

\subsection*{Problem 3}
\label{problem3}


To understand the bias of the NHST in Problem 2, we want to find a prior such that the posterior equials the $P$ value from Problem 2(a). 
Our null, $H_0$ is that the die is fair. 
We also consider the possibility of a loaded die ($Pr(1) = 1/2$). 

Specifically, we want to compute the value of $Pr(fair)$ such that

\[
Pr( fair | data)  = \frac{Pr(data | fair)Pr(fair)}{Pr(data|fair)Pr(fair)+ Pr(data|loaded)Pr(loaded)} = 0.017
\]

(Our $p$ value from Problem 2(a).)

Both $Pr(fair)$ and $Pr(loaded)$ are unknown, but sum to unity. 
Multiplying through by the denominator and simplyfying to solve for $Pr(fair)$ \dots

\begin{align*}
&Pr(fair) Pr(data | fair) = 0.017 \cdot \left(Pr(data|fair)Pr(fair)+ Pr(data|loaded)(1-Pr(fair))\right)\\
 &\implies Pr(fair) \left[ Pr(data|fair) \cdot (1 - 0.017) + 0.017 \cdot Pr(data|loaded)\right] = 0.017 \cdot Pr(data|loaded)\\
 &\implies Pr(fair) = \frac{0.017 \cdot Pr(data|loaded)}{\left[ Pr(data|fair) \cdot (1 - 0.017) + 0.017 \cdot Pr(data|loaded)\right]}
\end{align*}


<<bayes-pval-equivalent, message=FALSE>>=
pr.fair.pval = pd.pval * pr.data_loaded/( pr.data_fair*(1-pd.pval) + pd.pval*pr.data_loaded)
names(pr.fair.pval) = "fair"
pr.fair.pval
@ 

Thus, the prior probabilty of the null must be {\em very, very} low $Pr(H_0 | Data) = 2.6\times10^{-4}$ to obtain a prior probability equal to the $p$ value from above.

<<bayse-pval-check, message=FALSE>>=
post.check = pr.fair.pval*pr.data_fair/(pr.data_fair*pr.fair.pval + pr.data_loaded*(1-pr.fair.pval))
post.check
@ 




\subsection*{Colophon}

<<runit,eval=FALSE>>=
require(knitr) ### the package
knit('/Users/jaime/PHD/stat-rethink/hw2ashander.Rnw') ## to run
@ 


\end{document}
