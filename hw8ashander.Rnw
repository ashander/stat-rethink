\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
%\usepackage{breakurl}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\makeatother

\begin{document}

%% still can't get paths to work ...
\SweaveOpts{path=fig/hw7-,fig.align=center,fig.show=hold,dev=png,fig.width=6,fig.height=4}

<<setup,echo=FALSE,results=hide,message=FALSE>>=
options(replace.assign=TRUE,width=90)
knit_hooks$set(par=function(before, options, envir){if (before) par(mar=c(4,4,.1,.1),cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3,las=1)})
require(bbmle)
require(rethinking)
require(ggplot2)
require(emdbook)
@

\begin{center}
  {\bf \Large Homework 8, Statistical Rethinking}\\
\vspace{12pt}
   {\large Jaime Ashander}
\end{center}

\subsection*{Morality and predictors}

<<load, message=FALSE>>=

data(trolley)
d <- trolley
head(d,n=1)
@ 

\subsubsection*{(a)}


<<polr-gender,message=TRUE,cache=TRUE,warning=FALSE>>=
#m0 <- mle2(response ~ dordlogit(a=c(a1,a2,a3,a4,a5,a6), phi=bA*action + bI*intention), data=d, start=list(a1=-2,a2=-1,a3=0,a4=1,a5=2,a6=2.5,bA=0,bI=0))

m1 <- polr( as.ordered(response) ~ action * intention + contact * intention, data=d , Hess=TRUE )
m2 <- polr( as.ordered(response) ~ action * intention + contact * intention + male * contact , data=d , Hess=TRUE )
m3 <- polr( as.ordered(response) ~ action * intention + contact * intention + male  , data=d , Hess=TRUE )

compare(m1,m2,m3,nobs=nrow(d))

@ 

Model an interaction between gender and contact. 
Also look at a model without the interaction, but with gender. 

Gender improves the model. 
Model averaging is equivocal on the question of an interaction. 
Both AICc and BIC put some weight on each of the models with gender (with and without an interaction). 
BIC favors the interaction-less model, while AIC the other. 
As a conservative test, I'll use model averaging based on AICc (which seems to favor an interaction between contact and gender) to  check for support for the interaction in plotted results. 


<<trolley-plots,message=FALSE,cache=TRUE>>=
post <- sample.naive.posterior(list(m2,m3), n=2e4, method='AICc', nobs=nrow(d))
prob.indices <- grep("\\|", names(post2))

op <- par(mfrow=c(2,2))

for(kI in 0:1){
  for(kA in 0:1){
    kC <- 0:1 
    plot(1,1, type='n', xlab='contact', ylab='probability', xlim=c(0,1), ylim=c(0,1), xaxp=c(0,1,1), yaxp=c(0,1,2))
    col.vec=c('grey','black')
    ci.vec=c('black','white')

    title(paste('action',kA,'male 0:1','intent',kI))

    for(kM in 0:1){
      for(s in 1:200){
        p <- post[s,]
        ak <- as.numeric(p[prob.indices])
        phi <- with(p, kA*action + kI*intention + kC*contact + kA*kI*`action:intention` + kC*kI*`action:intention`+kM*kC*`contact:male` + kM*male)
        pk <- pordlogit(1:6, a=ak, phi=phi)
        for(i in 1:6)
          lines(kC, pk[,i], col=col.alpha(col.vec[kM+1], 0.01))
      }
      pmle <- as.list(coef(m2))
      if(class(m2)=='polr')
        ak <- m2$zeta
      phi <-  with(pmle, kA*action + kI*intention + kC*contact + kA*kI*`action:intention` + kC*kI*`action:intention`+kM*kC*`contact:male` + kM*male)
      pk.mle <- pordlogit(1:6, a=ak, phi=phi)
      for(i in 1:6) lines(0:1, pk.mle[,i], col=ci.vec[kM+1])
    }
  }
}
par(op)

@


Above depicts (model-averaged) effect of contact for female (light gray w/ black center lines) and male (dark gray w/ white centerlines). 
Males consistently respond more to contact scearios. 

<<interact-coef,message=TRUE,cache=TRUE,warning=FALSE>>=

op <- par(mfrow=c(1,2))
  dens(post$`contact:male`, main='AICc')
post <- sample.naive.posterior(list(m2,m3), n=2e4, method='BIC', nobs=nrow(d))
dens(post$`contact:male`, main='BIC')
par(op)
@ 
Model-averaged value for the interaction coefficient between {\tt contact} and {\tt male}. 


\subsubsection*{(b)}


Construct standardized predictors based on square and cube of age: 

<<polr-age,message=TRUE,cache=TRUE,warning=FALSE>>=
Standardize <- function(x) { 
  (x - mean(x))/sd(x)
}
d$age1 <- Standardize(d$age)
d$age2 <- Standardize(d$age^2) 
d$age3 <- Standardize(d$age^3) 

m4 <- polr( as.ordered(response) ~ action * intention + contact * intention + Standardize(age), data=d , Hess=TRUE )
m5 <- polr( as.ordered(response) ~ action * intention + contact * intention + Standardize(age) + Standardize(age^2), data=d , Hess=TRUE )
m6 <- polr( as.ordered(response) ~ action * intention + contact * intention + Standardize(age) + Standardize(age^2) + Standardize(age^3), data=d , Hess=TRUE )

compare(m4,m5,m6,nobs=nrow(d))

coeftab(m4,m5,m6)

precis(m6)

@ 

Model selection support a linear effect of age, with just a hint of a quadratic effect. 
In both cases the effect of age is negative, decreasing the tendancy toward Action, Intention or Contact. 

<<mod-avg-age,message=TRUE,cache=TRUE>>=
post2 <- sample.naive.posterior(list(m5,m6),method='AICc',nobs=nrow(d), n=2e4)
prob.indices <- grep("\\|", names(post2))

avg.coef <- colMeans(post2)


ages <- min(d$age):max(d$age)
for(kC in 0:1){
  op <- par(mfrow=c(2,2))
  for(kA in 0:1){
    for(kI in 0:1)
      {
        plot(1,1, xlab='age', ylab='probability', xlim=range(d$age), ylim=c(0,1), yaxp=c(0,1,2)) 

        title(paste('act',kA,'inten',kI,'cont',kC))

        for(s in 1:200){
          p <- post2[s,]
          ak <- as.numeric(p[prob.indices])
          phi <- with(p, kA*action + kI*intention + kC*contact + kA*kI*`action:intention` + kC*kI*`action:intention` + Standardize(ages)*`Standardize(age)` + Standardize(ages^2)*`Standardize(age^2)` + Standardize(ages^3)*`Standardize(age^3)`)
          pk <- pordlogit(1:6, a=ak, phi=phi)
          for(i in 1:6)
            lines(ages, pk[,i], col=col.alpha("grey", 0.01))
        }
        pmle <- as.list(avg.coef)
        ak <- avg.coef[prob.indices]
        phi <- with(pmle, kA*action + kI*intention + kC*contact + kA*kI*`action:intention` + kC*kI*`action:intention` + Standardize(ages)*`Standardize(age)` + Standardize(ages^2)*`Standardize(age^2)` + Standardize(ages^3)*`Standardize(age^3)`)    
        pk.mle <- pordlogit(1:6, a=ak, phi=phi)
        for(i in 1:6) lines(ages, pk.mle[,i], col="black")
      }
  }
  par(op)
}

@ 



\subsection*{Basketball}

<<load-b,message=FALSE>>=
d <- read.csv('fieldgoals0708.csv')
d <- d[order(d$FGM),]               
d$ID <- seq_along(d$player)
head(d)

@ 


<<beta-binom,message=FALSE>>=
dbetabinom2 <- function(x, pbar,  theta, size, log=FALSE){
  #using PMF from http://en.wikipedia.org/wiki/Beta-binomial_distribution with 
  n <- size
  k <- x
  a <- pbar*theta 
  b <- (1-pbar)*theta
  lp <- lchoose(n,k)+lbeta(k+a, n-k +b) - lbeta(a,b) #rewritten to log after looking at Bolker's emdbook::dbetabinom
  if(log)
    return(lp)
  else return(exp(lp))
  }

@ 

\subsubsection*{(a)}

For the beta-binomial model, fits to the raw data yielded poor estimates, with unrealistically low variance (0). 

<<binom,message=FALSE,cache=TRUE>>=
#min.goals <- min(d$FGM)
#d$FGM <-d$FGM - min.goals
#d$FGA <-d$FGA - min.goals

m2b <- mle2(FGM ~ dbinom(prob=logistic(a), size=FGA), data=d, start=list(a=0))
m2bb <- mle2(FGM ~ dbetabinom2(pbar=logistic(a), theta=theta, size=FGA), data=d, start=list(a=0, theta=2))
compare(m2b, m2bb, nobs=sum(d$FGA))
precis(m2bb)
@ 

<<post-bs,message=FALSE,cache=TRUE,warning=FALSE>>=
post.b1 <-sample.naive.posterior(m2b)
post.bb1 <- sample.naive.posterior(m2bb)
id.seq <- d$ID #seq(min(d$FGA), max(d$FGA), by=4)

mu <- sapply(id.seq, function(i) mean(rbinom(n=1e4, prob=logistic(post.b1$a), size=d$FGA[i])))
mu.ci <- sapply(id.seq, function(i) PCI(rbinom(n=1e4, prob=logistic(post.b1$a), size=d$FGA[i])))

Plot0 <- function(d, mu, mu.ci){
  plot(FGM/FGA~ID, data=d, xlab='Player ID',col=col.alpha('gray', 0.5), pch=19, ylim=c(0,1))
  lines(id.seq, mu.ci[1,]/d$FGA, lty=2)
  lines(id.seq, mu.ci[2,]/d$FGA, lty=2)
  lines(mu/FGA~id.seq, data=d)
}
op <- par(mfrow=c(2,2))
Plot0(d, mu, mu.ci)
dens(logistic(post.b1$a), xlim=c(.3,.6))
title('binomial probability')

prob <- rbeta2(n=1e4, prob=logistic(post.bb1$a), theta=post.bb1$theta)
mu <- sapply(id.seq, function(i) mean(rbinom(n=1e4, prob=prob, size=d$FGA[i])))
mu.ci <- sapply(id.seq, function(i) PCI(rbinom(n=1e4, prob=prob, size=d$FGA[i])))               

Plot0(d, mu, mu.ci)

prob.dens <- rbeta2(nrow(d)*100, prob=logistic(post.bb1$a), theta=post.bb1$theta)
dens(prob.dens, xlim=c(.3,.6))
title('beta-binomial probability')
par(op)
@ 

The above plots show field goals made over attempted plotted against player ID (assigned in order of shooting percentage). 
The first plot is the binomial model and the second is the beta-binomial.

The beta binomial distribution does a much better job capturing the spread in the data. 
This is because the mixture does not have the same constrained relationship between mean and variance as the binomial.

As we can see, however, there is a lot of heterogeneity in the players shooting percentage that is still unaccounted for by the mixture model. 
This is because the beta-binomial implemented above still assigns a constant (albeit beta-binomially distributed) mean. 

\subsubsection{(b)}

Still working with scaled attempts and successes, we fit the model 

(with FGA as predictor)

<<binom-w-pred,message=FALSE,cache=TRUE,warning=FALSE>>=
d <- read.csv('fieldgoals0708.csv')
d <- d[order(d$FGM),]               
d$ID <- seq_along(d$player)

#max.attempts <- max(d$FGA) 
m3b <- mle2(FGM ~ dbinom(prob=logistic(a+b*FGA/max(FGA)), size=FGA), data=d, start=list(a=0, b=0))
m3bb <- mle2(FGM ~ dbetabinom2(pbar=logistic(a+b*FGA/max(FGA)), theta=theta, size=FGA), data=d, start=list(a=0, b=0, theta=2))
precis(m3b)
precis(m3bb)

@


<<post-bs-w-pred,message=FALSE,cache=TRUE>>=
post.b2 <-sample.naive.posterior(m3b)
post.bb2 <- sample.naive.posterior(m3bb)
id.seq <- d$ID #seq(min(d$FGA), max(d$FGA), by=4)

post.b2$b <- post.b2$b/max(d$FGA) ## rescale posterior coefficient as in fitted equation
post.bb2$b <- post.bb2$b/max(d$FGA)

mu <- sapply(id.seq, function(i) mean(rbinom(n=1e4, prob=logistic(post.b2$a+post.b2$b*d$FGA[i]), size=d$FGA[i])))
mu.ci <- sapply(id.seq, function(i) PCI(rbinom(n=1e4, prob=logistic(post.b2$a+post.b2$b*d$FGA[i]), size=d$FGA[i])))

Plot1 <- function(POST, mu, mu.ci, D=d, betabin=FALSE){
  plot(FGM/FGA~FGA, data=D, pch='', ylim=c(0,1))
  for(i in 1:length(id.seq)) with(D, lines(rep(FGA[i],2), c(mu.ci[,i]/FGA[i]),col='lightgrey', lwd=1.5))
  points(FGM/FGA~FGA, data=D,col=col.alpha('darkgray', 0.8), pch=19)
  points(mu/FGA~FGA, data=D, pch=19, cex=0.5)

  if(betabin){
    prob.dens3 <- rbeta2(nrow(d)*100, prob=logistic(POST$a+ POST$b*d$FGA), theta=POST$theta)
    dens(prob.dens3,xlim=c(.3,.6))
    title('beta binomial probability')
    return(prob.dens3)
    }
  dens(with(POST,  logistic(a + b*with(D, mean(FGA)))),xlim=c(.3,.6))
  title('binomial probability')
  return(NULL)
}
op <- par(mfrow=c(2,2))
tmp <- Plot1(post.b2, mu, mu.ci, d)

mu <- sapply(id.seq, function(i) mean(rbinom(n=1e4, prob=rbeta2(n=1e4, prob=logistic(post.bb2$a+post.bb2$b*d$FGA[i]), theta=post.bb2$theta), size=d$FGA[i])))
mu.ci <- sapply(id.seq, function(i) PCI(rbinom(n=1e4, prob=rbeta2(n=1e4, prob=logistic(post.bb2$a+post.bb2$b*d$FGA[i]), theta=post.bb2$theta), size=d$FGA[i])))               

tmp <- Plot1(post.bb2, mu, mu.ci, d, TRUE)
  par(op)
@ 

Those taking more attempts have {\em a lower chance} of making the baskets.
Perhaps those who shoot more often shoot from further away?

\subsubsection{(c)}

<<binom-3pt,message=FALSE,cache=TRUE,warning=FALSE>>=
d <- read.csv('fieldgoals0708.csv')
d <- d[order(d$FGM),]               
d$ID <- seq_along(d$player)
d$TM <- d$FGM-d$X2PM
d$TA <- d$FGA-d$X2PA

m4b <- mle2(TM ~ dbinom(prob=logistic(a+bT*TA), size=TA), data=d, start=list(a=0, bT=0))
m4bb <- mle2(TM ~ dbetabinom2(pbar=logistic(a+bT*TA ), theta=exp(tau), size=TA), data=d, start=list(a=0, bT=0, tau=log(2)))
m4bbolker <- mle2(TM ~ dbetabinom(prob=logistic(a+bT*TA ), theta=exp(tau), size=TA), data=d, start=list(a=0, bT=0, tau=log(2)))
m4bb2 <- mle2(TM ~ dbetabinom2(pbar=logistic(a+bT*TA + bA*FGA), theta=exp(tau), size=TA), data=d, start=list(a=0, bT=0, bA=0, tau=log(2)))
precis(m4b)
precis(m4bb2)
compare(m4b, m4bb, m4bb2, nobs=sum(d$FGA))

@

No issues fitting, likely because of the exponential parameterization for $\theta$


<<post-bs-threes,message=FALSE,cache=TRUE,warning=FALSE>>=
post.b3 <-sample.naive.posterior(m4b)
post.bb3 <- sample.naive.posterior(m4bb)
id.seq <- d$ID #seq(min(d$TA), max(d$TA), by=4)

mu <- sapply(id.seq, function(i) mean(rbinom(n=1e4, prob=logistic(post.b3$a+post.b3$bT*d$TA[i]), size=d$TA[i])))
mu.ci <- sapply(id.seq, function(i) PCI(rbinom(n=1e4, prob=logistic(post.b3$a+post.b3$bT*d$TA[i]), size=d$TA[i])))


Plot2 <- function(POST, mu, mu.ci, D=d, betabin=FALSE){

  plot(TM/TA~log(TA), data=D, pch='', ylim=c(0,1))
  for(i in 1:length(id.seq)) with(D, lines(rep(log(TA[i]),2), c(mu.ci[,i]/TA[i]),col='lightgrey', lwd=1.5))
  points(TM/TA~log(TA), data=D,col=col.alpha('darkgray', 0.8), pch=19)
  points(mu/TA~log(TA), data=D, pch=19, cex=0.5)

  if(betabin){
    prob.dens3 <- rbeta2(nrow(d)*100, prob=logistic(POST$a+POST$bT*d$TA), theta=exp(POST$tau))
    dens(prob.dens3,xlim=c(.2,.5))
    title('beta binomial probability')
    return(prob.dens3)
    }
  dens(with(POST,  logistic(a + bT*with(D, mean(TA)))),xlim=c(.2,.5))
  title('binomial probability')
  return(NULL)
}
  op <- par(mfrow=c(2,2))
tmp <- Plot2(post.b3, mu, mu.ci, d)

mu <- sapply(id.seq, function(i) mean(rbinom(n=1e4, prob=rbeta2(n=1e4, prob=logistic(post.bb3$a+post.bb3$bT*d$TA[i]), theta=exp(post.bb3$tau)), size=d$TA[i])))
mu.ci <- sapply(id.seq, function(i) PCI(rbinom(n=1e4, prob=rbeta2(n=1e4, prob=logistic(post.bb3$a+post.bb3$bT*d$TA[i]), theta=exp(post.bb3$tau)), size=d$TA[i])))               

tmp <- Plot2(post.bb3, mu, mu.ci, d, TRUE)
par(op)
@ 


\subsection*{Colophon}

<<runit,eval=FALSE>>=
require(knitr) ### the package
knit(paste(getwd(),'hw8ashander.Rnw',sep='/')) ## to run1

##x to use all cores
require(snowfall)
sfInit(parallel=TRUE,cpus=4)
sfLibrary(rethinking)
sfExportAll()
sfStop()
@ 

<<other-ideas,eval=FALSE>>=
## make 'error bars'
#

@ 

\end{document}
